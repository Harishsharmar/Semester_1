{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "jYMlX8hlZBdB",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 41446,
     "status": "ok",
     "timestamp": 1642409653077,
     "user": {
      "displayName": "Harish Sharma",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03689107622530759000"
     },
     "user_tz": -330
    },
    "id": "jYMlX8hlZBdB",
    "outputId": "d80f3766-4ce9-4a63-869b-efb80caf7a17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
      "Get:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
      "Ign:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
      "Get:4 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
      "Hit:5 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
      "Ign:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
      "Get:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [696 B]\n",
      "Hit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
      "Get:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n",
      "Get:10 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
      "Hit:11 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
      "Get:12 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
      "Get:13 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease [15.9 kB]\n",
      "Get:14 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,498 kB]\n",
      "Hit:15 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
      "Get:16 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,459 kB]\n",
      "Get:17 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [725 kB]\n",
      "Get:19 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [867 kB]\n",
      "Get:20 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,823 kB]\n",
      "Get:21 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,935 kB]\n",
      "Get:22 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [934 kB]\n",
      "Get:23 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [758 kB]\n",
      "Get:24 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,238 kB]\n",
      "Get:25 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic/main amd64 Packages [45.3 kB]\n",
      "Fetched 14.6 MB in 4s (3,835 kB/s)\n",
      "Reading package lists... Done\n"
     ]
    }
   ],
   "source": [
    "!apt-get update\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "!wget -q http://archive.apache.org/dist/spark/spark-3.2.0/spark-3.2.0-bin-hadoop3.2.tgz\n",
    "!tar xf spark-3.2.0-bin-hadoop3.2.tgz\n",
    "!pip install -q findspark\n",
    "\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.0-bin-hadoop3.2\"\n",
    "\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1023a92",
   "metadata": {
    "executionInfo": {
     "elapsed": 7393,
     "status": "ok",
     "timestamp": 1642409663408,
     "user": {
      "displayName": "Harish Sharma",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03689107622530759000"
     },
     "user_tz": -330
    },
    "id": "e1023a92"
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('RDD Assignment').getOrCreate() # Creating Spark session object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13df87a5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 455,
     "status": "ok",
     "timestamp": 1642261189702,
     "user": {
      "displayName": "Harish Sharma",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03689107622530759000"
     },
     "user_tz": -330
    },
    "id": "13df87a5",
    "outputId": "7dc964f4-61f0-4b62-c859-e53f47cff9b9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.session.SparkSession"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c700a5f9",
   "metadata": {
    "id": "c700a5f9"
   },
   "source": [
    "#### Question 1 : Create RDDs in three different ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a793134",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2984,
     "status": "ok",
     "timestamp": 1642261228363,
     "user": {
      "displayName": "Harish Sharma",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03689107622530759000"
     },
     "user_tz": -330
    },
    "id": "0a793134",
    "outputId": "3331a153-b160-4049-d69d-844dfb311f46"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parallelize method :\n",
    "par_rdd = spark.sparkContext.parallelize(['hello','world','how','are','you ?','All is well'])\n",
    "par_rdd.collect()\n",
    "par_rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05mhWFaWbREC",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29857,
     "status": "ok",
     "timestamp": 1642361618633,
     "user": {
      "displayName": "Harish Sharma",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03689107622530759000"
     },
     "user_tz": -330
    },
    "id": "05mhWFaWbREC",
    "outputId": "537759fc-40e9-4786-bb42-964a7c880013"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f2aa2b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 556,
     "status": "ok",
     "timestamp": 1642261914873,
     "user": {
      "displayName": "Harish Sharma",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03689107622530759000"
     },
     "user_tz": -330
    },
    "id": "03f2aa2b",
    "outputId": "a0113db7-7cb0-4bb6-b93b-6b8d06a52305"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Harish is in ABD class.', 'Class is going on in ground floor']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RDD from Data source :\n",
    "ds_rdd = spark.sparkContext.textFile('input.txt')\n",
    "ds_rdd.collect()\n",
    "#ds_rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b703b4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 531,
     "status": "ok",
     "timestamp": 1642261946517,
     "user": {
      "displayName": "Harish Sharma",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03689107622530759000"
     },
     "user_tz": -330
    },
    "id": "64b703b4",
    "outputId": "f9285897-1ff3-4a24-b8d8-f999fb3d7d78"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create RDD using transformations :\n",
    "trans_rdd = ds_rdd.flatMap(lambda data:data.split(' '))\n",
    "trans_rdd.collect()\n",
    "trans_rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54dc405",
   "metadata": {
    "id": "a54dc405"
   },
   "source": [
    "#### Question 2 : Read a text file and count the number of words in the file using RDD operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53f606c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 491,
     "status": "ok",
     "timestamp": 1642261980353,
     "user": {
      "displayName": "Harish Sharma",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03689107622530759000"
     },
     "user_tz": -330
    },
    "id": "b53f606c",
    "outputId": "99cb9f75-4d4a-49e8-9955-feb5fedc0488"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_rdd = spark.sparkContext.textFile('input.txt')\n",
    "trans_rdd = ds_rdd.flatMap(lambda data:data.split(' '))\n",
    "trans_rdd.collect()\n",
    "trans_rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7edd6fb",
   "metadata": {
    "id": "b7edd6fb"
   },
   "source": [
    "####  Question 3 : Write a program to find the word frequency in a given file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024ea115",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 656,
     "status": "ok",
     "timestamp": 1642262042262,
     "user": {
      "displayName": "Harish Sharma",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03689107622530759000"
     },
     "user_tz": -330
    },
    "id": "024ea115",
    "outputId": "b34f2e86-9ebb-4f7d-e893-1055dec486b1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Harish', 1),\n",
       " ('is', 2),\n",
       " ('in', 2),\n",
       " ('ABD', 1),\n",
       " ('Class', 1),\n",
       " ('ground', 1),\n",
       " ('floor', 1),\n",
       " ('class.', 1),\n",
       " ('going', 1),\n",
       " ('on', 1)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_rdd.collect()\n",
    "word_rdd = trans_rdd.map(lambda data: (data,1)) # creating a tuple\n",
    "word1_rdd = word_rdd.reduceByKey(lambda a,b : a+b)\n",
    "word1_rdd.collect()\n",
    "#word1_rdd.count() # answer is 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e506bb64",
   "metadata": {
    "id": "e506bb64"
   },
   "source": [
    "#### Question 4 : Write a program to convert all words in a file to uppercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b85517",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1085,
     "status": "ok",
     "timestamp": 1642262160431,
     "user": {
      "displayName": "Harish Sharma",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03689107622530759000"
     },
     "user_tz": -330
    },
    "id": "e4b85517",
    "outputId": "b1472a5d-a9ad-4d55-ebcf-654de88418d6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HARISH',\n",
       " 'IS',\n",
       " 'IN',\n",
       " 'ABD',\n",
       " 'CLASS.',\n",
       " 'CLASS',\n",
       " 'IS',\n",
       " 'GOING',\n",
       " 'ON',\n",
       " 'IN',\n",
       " 'GROUND',\n",
       " 'FLOOR']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_rdd = spark.sparkContext.textFile('input.txt')\n",
    "upper_rdd = ds_rdd.flatMap(lambda data:data.split(' '))\n",
    "upper_rdd.collect()\n",
    "# def convert_to_upper(data) :\n",
    "#     if (data.isupper()) :\n",
    "#         return (data)\n",
    "#     else :\n",
    "#         return (data.upper())\n",
    "# upper1_rdd = upper_rdd.map(lambda data : convert_to_upper(data))\n",
    "upper1_rdd = upper_rdd.map(lambda data : data.upper())\n",
    "upper1_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9151515",
   "metadata": {
    "id": "a9151515"
   },
   "source": [
    "#### Question 5 : Write a program to convert all words in a file to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce7e68b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1681,
     "status": "ok",
     "timestamp": 1642263865323,
     "user": {
      "displayName": "Harish Sharma",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03689107622530759000"
     },
     "user_tz": -330
    },
    "id": "1ce7e68b",
    "outputId": "379b818a-5727-4648-f732-9ca43d93dbf8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['harish',\n",
       " 'is',\n",
       " 'in',\n",
       " 'abd',\n",
       " 'class.',\n",
       " 'class',\n",
       " 'is',\n",
       " 'going',\n",
       " 'on',\n",
       " 'in',\n",
       " 'ground',\n",
       " 'floor']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lower_rdd = upper_rdd.map(lambda data : data.lower())\n",
    "lower_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350e1b92",
   "metadata": {
    "id": "350e1b92"
   },
   "source": [
    "#### Question 6 : Write a program to capitalize first letter of each words in file (use string capitalize() method)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65054c26",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1253,
     "status": "ok",
     "timestamp": 1642264176890,
     "user": {
      "displayName": "Harish Sharma",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03689107622530759000"
     },
     "user_tz": -330
    },
    "id": "65054c26",
    "outputId": "9ae69fbf-62ed-4124-96fa-d1ab0be37a6d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Harish',\n",
       " 'Is',\n",
       " 'In',\n",
       " 'Abd',\n",
       " 'Class.',\n",
       " 'Class',\n",
       " 'Is',\n",
       " 'Going',\n",
       " 'On',\n",
       " 'In',\n",
       " 'Ground',\n",
       " 'Floor.']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''The capitalize() method returns a copy of the original string and converts the first character \n",
    "   of the string to a capital (uppercase) letter while making all other characters in the string lowercase letters.'''\n",
    "capital_rdd = upper1_rdd.map(lambda data : data.capitalize())\n",
    "capital_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa0a204",
   "metadata": {
    "id": "0fa0a204"
   },
   "source": [
    "#### Question 7 : Find the longest length of word from given set of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0813945c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 421,
     "status": "ok",
     "timestamp": 1642264614401,
     "user": {
      "displayName": "Harish Sharma",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03689107622530759000"
     },
     "user_tz": -330
    },
    "id": "0813945c",
    "outputId": "78d099d5-99a5-4527-ae3a-c631b4ab9790"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Harish is in ABD class.\n",
    "Class is going on in ground floor.\n",
    "Manipal is beautiful today.'''\n",
    "\n",
    "long_rdd = spark.sparkContext.textFile('input.txt')\n",
    "long1_rdd = long_rdd.flatMap(lambda data:data.split(' ')) \\\n",
    "            .map(lambda data : len(data))\n",
    "#long1_rdd.collect()\n",
    "#type(long1_rdd)\n",
    "#type(long1_rdd.collect())\n",
    "max(long1_rdd.collect())\n",
    "# RDD is not a list, but RDD.collect() is a list.\n",
    "# So max(long1_rdd) gives error but not max(long1_rdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cca422",
   "metadata": {
    "id": "11cca422"
   },
   "source": [
    "#### Question 8 : Map the Registration numbers to corresponding branch. 6000 series BDA, 9000 series HDA, 1000 series ML, 2000 series VLSI, 3000 series ES, 4000 series MSc, 5000 series CC. Given registration number, generate a key-value pair of Registration Number and Corresponding Branch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecf5b85",
   "metadata": {
    "id": "4ecf5b85",
    "outputId": "aa71fd90-e65b-4c17-f060-d9b835a9ab06"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ES', 5),\n",
       " ('MSc', 5),\n",
       " ('CC', 5),\n",
       " ('BDA', 5),\n",
       " ('ML', 5),\n",
       " ('VLSI', 5),\n",
       " ('HDA', 5)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_rdd = spark.sparkContext.textFile('D:/input.txt')\n",
    "reg_rdd = data_rdd.flatMap(lambda data : data.split('\\n'))\n",
    "reg_rdd.collect()\n",
    "# reg_rdd.count()  # 35\n",
    "\n",
    "def branch(data):\n",
    "    if(data[0:4] == '1000') :\n",
    "        return 'ML'\n",
    "    elif(data[0:4] == '2000') :\n",
    "        return 'VLSI'\n",
    "    elif(data[0:4] == '3000') :\n",
    "        return 'ES'\n",
    "    elif(data[0:4] == '4000') :\n",
    "        return 'MSc'\n",
    "    elif(data[0:4] == '5000') :\n",
    "        return 'CC'\n",
    "    elif(data[0:4] == '6000') :\n",
    "        return 'BDA'\n",
    "    elif(data[0:4] == '9000') :\n",
    "        return 'HDA'\n",
    "\n",
    "# Mapping registration number to its respective branch\n",
    "reg1_rdd = reg_rdd.map(lambda data : (data,branch(data)))\n",
    "reg1_rdd.collect()  \n",
    "# To find count in each branch\n",
    "reg1_rdd.map(lambda d : (d[1],1)).reduceByKey(lambda a,b : a+b).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca33bbf",
   "metadata": {
    "id": "4ca33bbf"
   },
   "source": [
    "#### Question 9 : Text file contain numbers. Numbers are separated by one white space. There is no order to store the numbers. One line may contain one or more numbers. Find the maximum, minimum, sum and mean of numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da3e293",
   "metadata": {
    "id": "2da3e293",
    "outputId": "05aa5e94-72e9-48c6-ae71-55b2a707cbeb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "269.42105263157896"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_rdd = spark.sparkContext.textFile('C:/Users/MSIS/Desktop/numbers.txt')\n",
    "num_rdd = num_rdd.flatMap(lambda data: data.split(' '))\n",
    "num_rdd.collect()\n",
    "max(num_rdd.map(lambda data : int(data)).collect())\n",
    "#num_rdd.map(lambda data : int(data)).max()\n",
    "min(num_rdd.map(lambda data : int(data)).collect())\n",
    "#num_rdd.map(lambda data : int(data)).min()\n",
    "sum(num_rdd.map(lambda data : int(data)).collect())\n",
    "#num_rdd.map(lambda data : int(data)).sum()\n",
    "num_rdd.map(lambda data : int(data)).mean()\n",
    "#num_rdd.flatMap(lambda data: data.split(' ')).map(lambda data : int(data)).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901c80d1",
   "metadata": {
    "id": "901c80d1"
   },
   "source": [
    "#### Question 10 : A text file (citizen.txt) contains data about citizens of country. Fields (information in file) are Name, dob, Phone, email and state name. Another file contains mapping of state names to state code like Karnataka is codes as KA, TamilNadu as TN, Kerala KL etc. Compress the citizen.txt file by changing full state name to state code. Write compressed data back into new file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2033c646",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 617,
     "status": "ok",
     "timestamp": 1642268824083,
     "user": {
      "displayName": "Harish Sharma",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03689107622530759000"
     },
     "user_tz": -330
    },
    "id": "2033c646",
    "outputId": "925a450d-3519-4cb0-a498-ee4659ca71a5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Karnataka', 'KA'],\n",
       " ['Himachal_Pradesh', 'HP'],\n",
       " ['Delhi', 'DL'],\n",
       " ['Maharashtra', 'MH'],\n",
       " ['Tamil_Nadu', 'TN'],\n",
       " ['West_Bengal', 'WB'],\n",
       " ['Rajasthan', 'RJ']]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating RDD for the file which contains state name and its respective code :\n",
    "# State code mapping\n",
    "# This RDD should be broadcasted to all the systems doing the mapping of citizen_rdd.\n",
    "state_rdd = spark.sparkContext.textFile('state_mapping.txt')\\\n",
    "        .map(lambda d:d.split(' '))\n",
    "state_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6ae716",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 589,
     "status": "ok",
     "timestamp": 1642268862707,
     "user": {
      "displayName": "Harish Sharma",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03689107622530759000"
     },
     "user_tz": -330
    },
    "id": "ca6ae716",
    "outputId": "8d6e5c49-11f4-403d-f697-682f6c43b6d0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Delhi': 'DL',\n",
       " 'Himachal_Pradesh': 'HP',\n",
       " 'Karnataka': 'KA',\n",
       " 'Maharashtra': 'MH',\n",
       " 'Rajasthan': 'RJ',\n",
       " 'Tamil_Nadu': 'TN',\n",
       " 'West_Bengal': 'WB'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict = {} # Creating empty dictionary\n",
    "# state_dict is of dict type\n",
    "for ele in state_rdd.collect():\n",
    "    state_dict[ele[0]]=ele[1]  # Assigning values to key (Value is KA and key is Karnataka)\n",
    "state_code = spark.sparkContext.broadcast(state_dict)\n",
    "# state_code should be of broadcast type\n",
    "#type(state_dict)  : Dict type\n",
    "#type(state_code) : broadcast type\n",
    "state_code\n",
    "state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e864a971",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 465,
     "status": "ok",
     "timestamp": 1642268879101,
     "user": {
      "displayName": "Harish Sharma",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03689107622530759000"
     },
     "user_tz": -330
    },
    "id": "e864a971",
    "outputId": "e7730676-4dc9-4af8-b9f6-65e5f0828e3b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Harish R Sharma 02-01-1997 9845724355 hrs@gmail.com Karnataka',\n",
       " 'Girish Shetty 03-08-2001 2348910233 gir123@gmail.com Maharashtra',\n",
       " 'Mahesh Kumar 12-10-1978 0192384718 sdklfj@dllk.co.uk Delhi',\n",
       " 'Satish Bhandary U A 31-9-2000 9871234811 dlkfsj@k;dlf.com Himachal_Pradesh',\n",
       " 'Navami Patel 21-3-96 3928479823 navanavanavami@navanami.msnsjd Tamil_Nadu',\n",
       " 'Greeshma Unnikrishnan M 4-06-2004 9871237865 lksjdffdhj@jdhfad.yg West_Bengal',\n",
       " 'Pooja Gupta 19-05-1995 8746387912 jkhfdaf@kjad.com Rajasthan']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Original dataset\n",
    "citizen_rdd = spark.sparkContext.textFile('citizen.txt')\n",
    "citizen_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4483525",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 458,
     "status": "ok",
     "timestamp": 1642269532454,
     "user": {
      "displayName": "Harish Sharma",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03689107622530759000"
     },
     "user_tz": -330
    },
    "id": "e4483525",
    "outputId": "7312b584-3eb4-4e22-9764-3632574fd89d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Harish,R,Sharma,02-01-1997,9845724355,hrs@gmail.com,KA',\n",
       " 'Girish,Shetty,03-08-2001,2348910233,gir123@gmail.com,MH',\n",
       " 'Mahesh,Kumar,12-10-1978,0192384718,sdklfj@dllk.co.uk,DL',\n",
       " 'Satish,Bhandary,U,A,31-9-2000,9871234811,dlkfsj@k;dlf.com,HP',\n",
       " 'Navami,Patel,21-3-96,3928479823,navanavanavami@navanami.msnsjd,TN',\n",
       " 'Greeshma,Unnikrishnan,M,4-06-2004,9871237865,lksjdffdhj@jdhfad.yg,WB',\n",
       " 'Pooja,Gupta,19-05-1995,8746387912,jkhfdaf@kjad.com,RJ']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Passing entire row of a file into a function to do splitting and necessary overwriting\n",
    "def map_state_code(data):\n",
    "    data = data.split(' ')\n",
    "    data[-1] = state_code.value.get(data[-1]) # last column is of interest here\n",
    "    new_data = ','\n",
    "    new_data = new_data.join(data)\n",
    "    #new_data = ' '.join(data)\n",
    "    return new_data\n",
    "\n",
    "new_citizen = citizen_rdd.map(lambda d:map_state_code(d)) # State name is replaced by state code\n",
    "new_citizen.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc770e7",
   "metadata": {
    "id": "cdc770e7"
   },
   "outputs": [],
   "source": [
    "# Writing back the modified output to a new folder (Output inside folder will be in part files)\n",
    "# Only one part file because repartition(1)\n",
    "new_citizen.repartition(1).saveAsTextFile('C:/Users/MSIS/BDA21-SparkExamples/replaced_file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e306fd25",
   "metadata": {
    "id": "e306fd25",
    "outputId": "f570f48e-14ef-491e-81ab-0941793bfa96"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.broadcast.Broadcast"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(state_dict)\n",
    "type(state_code) # State_code is of broadcast type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc456c8",
   "metadata": {
    "id": "5fc456c8"
   },
   "source": [
    "##### Question 11 : Create dataset (text file) with fields like ‘Student Name’, ‘Institute’, ‘Program Name’, and ‘Gender’ and solve following questions.\n",
    "##### 1. Compute number of students from each Institute.\n",
    "##### 2. Number of students enrolled to any program.\n",
    "##### 3. Number of ‘boy’ and ‘girl’ students.\n",
    "##### 4. Number of ‘boy’ and ‘girl’ students from selected Institute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8832e83",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1725,
     "status": "ok",
     "timestamp": 1642356494138,
     "user": {
      "displayName": "Harish Sharma",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03689107622530759000"
     },
     "user_tz": -330
    },
    "id": "f8832e83",
    "outputId": "05e8a845-22a9-4b2c-d722-0727c0860b09"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Male', 1), ('Female', 0)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Harish MSIS BDA Male\n",
    "Prathik MIT ECE Male\n",
    "Lavanya MSC Chemistry Female\n",
    "Prathima MIT CSE Female\n",
    "Kavvya MSIS AIML Female\n",
    "Girish SMK Physics Male'''\n",
    "\n",
    "student_rdd = spark.sparkContext.textFile('student.txt')\n",
    "student_rdd.collect()\n",
    "\n",
    "student1_rdd = student_rdd.map(lambda d:d.split(' ')).map(lambda d:(d[1],1)).reduceByKey(lambda a,b:a+b)\n",
    "student1_rdd.collect()\n",
    "\n",
    "student2_rdd = student_rdd.map(lambda d:d.split(' ')).map(lambda d:(d[2],1)).reduceByKey(lambda a,b:a+b)\n",
    "student2_rdd.collect()\n",
    "\n",
    "student3_rdd = student_rdd.map(lambda d:d.split(' ')).map(lambda d:(d[3],1)).reduceByKey(lambda a,b:a+b)\n",
    "student3_rdd.collect()\n",
    "\n",
    "student4_rdd = student_rdd.map(lambda d:d.split(' ')).map(lambda d:(d[3],1) if(d[1]=='SMK') else (d[3],0)).reduceByKey(lambda a,b:a+b)\n",
    "student4_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iFJKH5vUYrDM",
   "metadata": {
    "id": "iFJKH5vUYrDM"
   },
   "source": [
    "##### Question 12 : \n",
    "##### Dataset: Temperature of Indian Cities. Fields of dataset are Date, Average Temperature, City, Country, Latitude and Longitude (Use dataset attached to MapReduce assignment). Solve following questions\n",
    "##### 1. Find maximum and minimum temperature of all cities from the given dataset\n",
    "##### 2. Count number of data point for each city.\n",
    "##### 3. Find the maximum and minimum temperature for city Bangalore from the given dataset.\n",
    "##### 4. Find the maximum and minimum temperature for any given city from the given dataset. City name should be passed through command line argument. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FMn3zknWGWCw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6618,
     "status": "ok",
     "timestamp": 1642361472971,
     "user": {
      "displayName": "Harish Sharma",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03689107622530759000"
     },
     "user_tz": -330
    },
    "id": "FMn3zknWGWCw",
    "outputId": "73905555-22b6-4f94-d8e1-5f0613745177"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Bangalore', '29.688000000000002')]\n",
      "[('Bangalore', '20.257')]\n"
     ]
    }
   ],
   "source": [
    "temp_rdd = spark.sparkContext.textFile('sampleTemp.txt').map(lambda d:d.split(' '))\n",
    "#temp_rdd.collect()\n",
    "\n",
    "# Max and Min :\n",
    "temp_max = temp_rdd.map(lambda d:(d[2],d[1])).reduceByKey(lambda a,b : max(a,b))\n",
    "temp_max.collect()\n",
    "temp_min = temp_rdd.map(lambda d:(d[2],d[1])).reduceByKey(lambda a,b : min(a,b))\n",
    "temp_min.collect()\n",
    "\n",
    "# Count :\n",
    "count_city = temp_rdd.map(lambda d:(d[2],1)).reduceByKey(lambda a,b : a+b)\n",
    "count_city.collect()\n",
    "\n",
    "# Max and Min for Bangalore :\n",
    "blore_max = temp_rdd.map(lambda d:(d[2],d[1])).filter(lambda d:(d[0]==\"Bangalore\")).reduceByKey(lambda a,b : max(a,b))\n",
    "print(blore_max.collect())\n",
    "blore_min = temp_rdd.map(lambda d:(d[2],d[1])).filter(lambda d:(d[0]==\"Bangalore\")).reduceByKey(lambda a,b : min(a,b))\n",
    "print(blore_min.collect())\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZG38oHaaY6jN",
   "metadata": {
    "id": "ZG38oHaaY6jN"
   },
   "source": [
    "##### Question 13 : Create dataset (text file) of bank transactions. Fields in file are ‘Bank ID’, ‘Account Number’, ‘Transaction Date’, ‘Transaction Type’ (credit or debit), ‘Transaction Amount’. Date format is dd-mm-yyyy.\n",
    "##### 1. Count unique number of customers\n",
    "##### 2. Count unique number of Bank ID\n",
    "##### 3. Count unique number of customers per Bank ID\n",
    "##### 4. Number of transactions for given Account Number\n",
    "##### 5. Number of credit transactions for given Account Number in a given year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "Pek8S0rPSB_M",
   "metadata": {
    "executionInfo": {
     "elapsed": 613,
     "status": "ok",
     "timestamp": 1642410364980,
     "user": {
      "displayName": "Harish Sharma",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03689107622530759000"
     },
     "user_tz": -330
    },
    "id": "Pek8S0rPSB_M"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "1001 211058027 21-06-2011 Cr 1012.23\n",
    "2012 211012078 23-11-1978 Dr 20123.34\n",
    "1399 132907341 11-03-2020 Cr 291.22\n",
    "9876 093092223 09-12-1918 Dr 93180\n",
    "1096 211058027 11-06-2000 Cr 1093.96\n",
    "2012 132907341 14-06-2002 Cr 21191.22\n",
    "9876 093092223 09-12-2018 Dr 93.913\n",
    "'''\n",
    "bank_data = spark.sparkContext.textFile('bank.txt').map(lambda d:d.split('\\t'))\n",
    "#columns = \"`Bank_ID` STRING, `Account_Number` STRING, `Transaction_Date` STRING, `Transaction_Type` STRING, `Amount` STRING\"\n",
    "#bankDF = spark.createDataFrame(bank_data,columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "GEb6hH14i1CO",
   "metadata": {
    "executionInfo": {
     "elapsed": 690,
     "status": "ok",
     "timestamp": 1642410796974,
     "user": {
      "displayName": "Harish Sharma",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03689107622530759000"
     },
     "user_tz": -330
    },
    "id": "GEb6hH14i1CO"
   },
   "outputs": [],
   "source": [
    "# bankDF.show()\n",
    "# bankDF.printSchema()\n",
    "#bank_data.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "OeruqpC9jeRF",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 454,
     "status": "ok",
     "timestamp": 1642410961986,
     "user": {
      "displayName": "Harish Sharma",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03689107622530759000"
     },
     "user_tz": -330
    },
    "id": "OeruqpC9jeRF",
    "outputId": "fc931ac1-52b8-45a9-ed87-240cb27caa21"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('123876122', 4),\n",
       " ('123872822', 2),\n",
       " ('123876171', 2),\n",
       " ('123876142', 4),\n",
       " ('123823122', 1),\n",
       " ('123809822', 1),\n",
       " ('123376422', 1),\n",
       " ('123802671', 1),\n",
       " ('123823822', 2),\n",
       " ('123872322', 3),\n",
       " ('123872829', 6),\n",
       " ('720127541', 2),\n",
       " ('123456789', 5),\n",
       " ('123822322', 1),\n",
       " ('123830822', 1)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count unique number of customers\n",
    "# bankDF.select('Bank_ID','Account_Number').groupBy('Bank_ID').count().collect() (Wrong approach)\n",
    "\n",
    "bank_data1 = bank_data.map(lambda d:(d[1],1)).reduceByKey(lambda a,b:a+b)\n",
    "bank_data1.collect()\n",
    "#bank_data1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "S4k5WYmAmXjt",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 454,
     "status": "ok",
     "timestamp": 1642411019514,
     "user": {
      "displayName": "Harish Sharma",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03689107622530759000"
     },
     "user_tz": -330
    },
    "id": "S4k5WYmAmXjt",
    "outputId": "b7b057ba-22d7-43fb-e3ad-7712e1187a91"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('CAN00123', 12), ('SBI00042', 18), ('ICI00072', 6)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count unique number of Bank ID :\n",
    "bank_data2 = bank_data.map(lambda d:(d[0],1)).reduceByKey(lambda a,b:a+b)\n",
    "bank_data2.collect()\n",
    "#bank_data2.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LnhS-KIyU5Hp",
   "metadata": {
    "id": "LnhS-KIyU5Hp"
   },
   "source": [
    "------------------------------------------------------------------------\n",
    "------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cNuWQX-P0C2c",
   "metadata": {
    "executionInfo": {
     "elapsed": 399,
     "status": "ok",
     "timestamp": 1642403797805,
     "user": {
      "displayName": "Harish Sharma",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03689107622530759000"
     },
     "user_tz": -330
    },
    "id": "cNuWQX-P0C2c"
   },
   "outputs": [],
   "source": [
    "# bank_data_3 = bankDF.select('Bank_ID','Account_Number').groupBy('Bank_ID').count()\n",
    "# bank_data_3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3sghWhVG0jGE",
   "metadata": {
    "executionInfo": {
     "elapsed": 373,
     "status": "ok",
     "timestamp": 1642403804932,
     "user": {
      "displayName": "Harish Sharma",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03689107622530759000"
     },
     "user_tz": -330
    },
    "id": "3sghWhVG0jGE"
   },
   "outputs": [],
   "source": [
    "# bank_data_3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "A2OX0SIem3yO",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1642404095024,
     "user": {
      "displayName": "Harish Sharma",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03689107622530759000"
     },
     "user_tz": -330
    },
    "id": "A2OX0SIem3yO"
   },
   "outputs": [],
   "source": [
    "# Count unique number of customers per bank ID :\n",
    "# bank_data3 = bank_data.map(lambda d:(d[0],d[1]))\n",
    "# bank1 = bank_data3.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9CYQUHQI5VcZ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 505,
     "status": "ok",
     "timestamp": 1642405286179,
     "user": {
      "displayName": "Harish Sharma",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03689107622530759000"
     },
     "user_tz": -330
    },
    "id": "9CYQUHQI5VcZ",
    "outputId": "4c533831-c08a-45b8-9b3f-086eec798c0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+\n",
      "|  _1|       _2|\n",
      "+----+---------+\n",
      "|1001|211058027|\n",
      "|2012|211012078|\n",
      "|1399|132907341|\n",
      "|9876|093092223|\n",
      "|1096|211058027|\n",
      "|2012|132907341|\n",
      "|9876|093092223|\n",
      "+----+---------+\n",
      "\n",
      "+----+---------+\n",
      "|  _1|       _2|\n",
      "+----+---------+\n",
      "|1001|211058027|\n",
      "|1096|211058027|\n",
      "|1399|132907341|\n",
      "|2012|132907341|\n",
      "|2012|211012078|\n",
      "|9876|093092223|\n",
      "|9876|093092223|\n",
      "+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# bank1.show()\n",
    "# bank1.createOrReplaceTempView(\"employee\")  # Temporary view of filtered dataframe having name as 'employee'\n",
    "# sqlDF = spark.sql(\"SELECT * FROM employee\")\n",
    "# sqlDF.orderBy('_1').show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zR1I5_RGVG5w",
   "metadata": {
    "id": "zR1I5_RGVG5w"
   },
   "source": [
    "------------------------------------------------------------------------\n",
    "------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "BJLLY6Cu5cJu",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 409,
     "status": "ok",
     "timestamp": 1642411690605,
     "user": {
      "displayName": "Harish Sharma",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03689107622530759000"
     },
     "user_tz": -330
    },
    "id": "BJLLY6Cu5cJu",
    "outputId": "31cd7962-b070-4811-9ed8-ef5b9560c083"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('CAN00123',\n",
       "  {'123456789',\n",
       "   '123823822',\n",
       "   '123872822',\n",
       "   '123876122',\n",
       "   '123876171',\n",
       "   '720127541'}),\n",
       " ('SBI00042',\n",
       "  {'123823822',\n",
       "   '123872322',\n",
       "   '123872822',\n",
       "   '123872829',\n",
       "   '123876122',\n",
       "   '123876142',\n",
       "   '123876171'}),\n",
       " ('ICI00072',\n",
       "  {'123376422',\n",
       "   '123802671',\n",
       "   '123809822',\n",
       "   '123822322',\n",
       "   '123823122',\n",
       "   '123830822'})]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count unique number of customers per bank ID\n",
    "bank_data.map(lambda data: (data[0],data[1])).groupByKey().map(lambda tup: (tup[0],set(tup[1]))).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "elps3xsfohJ9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 470,
     "status": "ok",
     "timestamp": 1642411836459,
     "user": {
      "displayName": "Harish Sharma",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03689107622530759000"
     },
     "user_tz": -330
    },
    "id": "elps3xsfohJ9",
    "outputId": "e195c651-be14-45d6-86a9-868543046a14"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('CAN00123', 6), ('SBI00042', 7), ('ICI00072', 6)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bank_data3 = bank_data.map(lambda data: (data[0],data[1])).groupByKey().map(lambda tup: (tup[0],len(set(tup[1]))))\n",
    "bank_data3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "Db-iy49dYJhC",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 657,
     "status": "ok",
     "timestamp": 1642411996809,
     "user": {
      "displayName": "Harish Sharma",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03689107622530759000"
     },
     "user_tz": -330
    },
    "id": "Db-iy49dYJhC",
    "outputId": "bfaeb6ab-c611-4d3e-9565-0989574accfe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('123876122', 4),\n",
       " ('123872822', 2),\n",
       " ('123876171', 2),\n",
       " ('123876142', 4),\n",
       " ('123823122', 1),\n",
       " ('123809822', 1),\n",
       " ('123376422', 1),\n",
       " ('123802671', 1),\n",
       " ('123823822', 2),\n",
       " ('123872322', 3),\n",
       " ('123872829', 6),\n",
       " ('720127541', 2),\n",
       " ('123456789', 5),\n",
       " ('123822322', 1),\n",
       " ('123830822', 1)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of transactions for given Account Number\n",
    "bank_data4 = bank_data.map(lambda d:(d[1],1)).reduceByKey(lambda a,b : a+b)\n",
    "bank_data4.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "_RAnT32eZWbh",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 440,
     "status": "ok",
     "timestamp": 1642414752623,
     "user": {
      "displayName": "Harish Sharma",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03689107622530759000"
     },
     "user_tz": -330
    },
    "id": "_RAnT32eZWbh",
    "outputId": "6fe61bed-6da5-401d-8f8c-142caaf899e9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('123876171', '2021', 'D'), ('123876171', '2005', 'D')]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of Credit transactions for given account number in a given year\n",
    "#bank_data.map(lambda d:(d[1],d[2],d[3])).collect()\n",
    "bank_data.map(lambda d:(d[1],d[2].split('-')[2],d[3])).filter(lambda d : (d[0]==\"123876171\")).collect()\n",
    "#.map(lambda d : (d[2],1).filter.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8dCg6mF-hbii",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 435,
     "status": "ok",
     "timestamp": 1642414757502,
     "user": {
      "displayName": "Harish Sharma",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03689107622530759000"
     },
     "user_tz": -330
    },
    "id": "8dCg6mF-hbii",
    "outputId": "d04414a5-e9b2-4751-b2e0-6ae9636a79e1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bank_data.map(lambda d:(d[1],d[2].split('-')[2],d[3])).filter(lambda d : ((d[0]==\"123876171\") & (d[1]==\"2021\") & (d[2]==\"C\"))).count()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "RDD assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
